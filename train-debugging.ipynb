{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hello world\t\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('hello world')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "--[[\n",
    "Main entry point for training a DenseCap model\n",
    "]]--\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "-- Includes\n",
    "-------------------------------------------------------------------------------\n",
    "require 'torch'\n",
    "require 'nngraph'\n",
    "require 'optim'\n",
    "require 'image'\n",
    "require 'lfs'\n",
    "require 'nn'\n",
    "local cjson = require 'cjson'\n",
    "\n",
    "require 'densecap.DataLoader_new'\n",
    "require 'densecap.DenseCapModel'\n",
    "require 'densecap.optim_updates'\n",
    "local utils = require 'densecap.utils'\n",
    "local utils = require 'densecap.utils'\n",
    "local opts = require 'train_opts'\n",
    "local models = require 'models'\n",
    "local eval_utils = require 'eval.eval_utils'\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "-- Initializations\n",
    "-------------------------------------------------------------------------------\n",
    "local opt = opts.parse(arg)\n",
    "print(opt)\n",
    "torch.setdefaulttensortype('torch.FloatTensor')\n",
    "torch.manualSeed(opt.seed)\n",
    "if opt.gpu >= 0 then\n",
    "  -- cuda related includes and settings\n",
    "  require 'cutorch'\n",
    "  require 'cunn'\n",
    "  require 'cudnn'\n",
    "  cutorch.manualSeed(opt.seed)\n",
    "  cutorch.setDevice(opt.gpu + 1) -- note +1 because lua is 1-indexed\n",
    "end\n",
    "\n",
    "-- initialize the data loader class\n",
    "local loader = DataLoader(opt)\n",
    "opt.num_classes = loader:getNumClasses()\n",
    "opt.idx_to_cls = loader.info.idx_to_cls\n",
    "\n",
    "-- initialize the DenseCap model object\n",
    "local dtype = 'torch.CudaTensor'\n",
    "local model = models.setup(opt):type(dtype)\n",
    "\n",
    "-- get the parameters vector\n",
    "local params, grad_params, cnn_params, cnn_grad_params = model:getParameters()\n",
    "print('total number of parameters in net: ', grad_params:nElement())\n",
    "print('total number of parameters in CNN: ', cnn_grad_params:nElement())\n",
    "\n",
    "-- Initialize training information\n",
    "local loss_history = {}\n",
    "local all_losses = {}\n",
    "local results_history = {}\n",
    "local iter = 1\n",
    "local optim_state = {}\n",
    "local cnn_optim_state = {}\n",
    "local best_val_score\n",
    "if string.len(opt.checkpoint_start_from) > 0 then\n",
    "  -- load protos from file\n",
    "  print('initializing training information from ' .. opt.checkpoint_start_from)\n",
    "  local loaded_checkpoint = torch.load(opt.checkpoint_start_from)\n",
    "  iter = loaded_checkpoint.iter + 1 or iter\n",
    "  loss_history = loaded_checkpoint.loss_history or loss_history\n",
    "  all_losses = loaded_checkpoint.all_losses or all_losses\n",
    "  results_history = loaded_checkpoint.results_history or results_history\n",
    "  optim_state = loaded_checkpoint.optim_state or optim_state\n",
    "  cnn_optim_state = loaded_checkpoint.cnn_optim_state or cnn_optim_state\n",
    "  if opt.load_best_score == 1 then\n",
    "    best_val_score = loaded_checkpoint.best_val_score\n",
    "  end\n",
    "  loader.iterators = loaded_checkpoint.iterators or loader.iterators\n",
    "end\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "-- Loss function\n",
    "-------------------------------------------------------------------------------\n",
    "\n",
    "local function lossFun()\n",
    "  grad_params:zero()\n",
    "  if opt.finetune_cnn_after ~= -1 and iter >= opt.finetune_cnn_after then\n",
    "    cnn_grad_params:zero() \n",
    "  end\n",
    "  model:training()\n",
    "\n",
    "  -- Fetch data using the loader\n",
    "  local timer = torch.Timer()\n",
    "  local info\n",
    "  local data = {}\n",
    "  local loading_time = utils.timeit(function()\n",
    "    data.image, data.gt_boxes, data.gt_labels, info, data.region_proposals = loader:getBatch()\n",
    "  end)\n",
    "  print('Loading batch time:\\t' .. loading_time)\n",
    "  -- data.image, data.gt_boxes, data.gt_labels, info, data.region_proposals = loader:getBatch()\n",
    "  for k, v in pairs(data) do\n",
    "    data[k] = v:type(dtype)\n",
    "  end\n",
    "  if opt.timing then cutorch.synchronize() end\n",
    "  local getBatch_time = timer:time().real\n",
    "\n",
    "  -- Run the model forward and backward\n",
    "  model.timing = opt.timing\n",
    "  model.cnn_backward = false\n",
    "  if opt.finetune_cnn_after ~= -1 and iter > opt.finetune_cnn_after then\n",
    "    model.finetune_cnn = true\n",
    "  end\n",
    "  model.dump_vars = false\n",
    "  if opt.progress_dump_every > 0 and iter % opt.progress_dump_every == 0 then\n",
    "    model.dump_vars = true\n",
    "  end\n",
    "  local losses, stats\n",
    "  local fb_time = utils.timeit(function()\n",
    "    losses, stats = model:forward_backward(data)\n",
    "  end)\n",
    "  print('Forward-backward time:\\t' .. fb_time)\n",
    "  -- local losses, stats = model:forward_backward(data)\n",
    "\n",
    "  -- Apply L2 regularization\n",
    "  if opt.weight_decay > 0 then\n",
    "    grad_params:add(opt.weight_decay, params)\n",
    "    if cnn_grad_params then cnn_grad_params:add(opt.weight_decay, cnn_params) end\n",
    "  end\n",
    "\n",
    "  --+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "  -- Visualization/Logging code\n",
    "  --+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++\n",
    "  if opt.losses_log_every > 0 and iter % opt.losses_log_every == 0 then\n",
    "    local losses_copy = {}\n",
    "    for k, v in pairs(losses) do losses_copy[k] = v end\n",
    "    loss_history[iter] = losses_copy\n",
    "  end\n",
    "\n",
    "  return losses, stats\n",
    "end\n",
    "\n",
    "-------------------------------------------------------------------------------\n",
    "-- Main loop\n",
    "-------------------------------------------------------------------------------\n",
    "local loss0\n",
    "while true do  \n",
    "\n",
    "  -- Compute loss and gradient\n",
    "  local losses, stats = lossFun()\n",
    "\n",
    "  -- Parameter update\n",
    "  -- perform a parameter update\n",
    "  if opt.optim == 'rmsprop' then\n",
    "    rmsprop(params, grad_params, opt.learning_rate, opt.optim_alpha, opt.optim_epsilon, optim_state)\n",
    "  elseif opt.optim == 'adagrad' then\n",
    "    adagrad(params, grad_params, opt.learning_rate, opt.optim_epsilon, optim_state)\n",
    "  elseif opt.optim == 'sgd' then\n",
    "    sgd(params, grad_params, opt.learning_rate)\n",
    "  elseif opt.optim == 'sgdm' then\n",
    "    sgdm(params, grad_params, opt.learning_rate, opt.optim_alpha, optim_state)\n",
    "  elseif opt.optim == 'sgdmom' then\n",
    "    sgdmom(params, grad_params, opt.learning_rate, opt.optim_alpha, optim_state)\n",
    "  elseif opt.optim == 'adam' then\n",
    "    adam(params, grad_params, opt.learning_rate, opt.optim_alpha, opt.optim_beta, opt.optim_epsilon, optim_state)\n",
    "  else\n",
    "    error('bad option opt.optim')\n",
    "  end\n",
    "\n",
    "  -- Make a step on the CNN if finetuning\n",
    "  if opt.finetune_cnn_after >= 0 and iter >= opt.finetune_cnn_after then\n",
    "    if opt.cnn_optim == 'sgd' then\n",
    "      sgd(cnn_params, cnn_grad_params, opt.cnn_learning_rate)\n",
    "    elseif opt.cnn_optim == 'sgdm' then\n",
    "      sgdm(cnn_params, cnn_grad_params, opt.cnn_learning_rate, opt.cnn_optim_alpha, cnn_optim_state)\n",
    "    elseif opt.cnn_optim == 'sgdmom' then\n",
    "      sgdmom(cnn_params, cnn_grad_params, opt.cnn_learning_rate, opt.cnn_optim_alpha, cnn_optim_state)\n",
    "    elseif opt.cnn_optim == 'adam' then\n",
    "      adam(cnn_params, cnn_grad_params, opt.cnn_learning_rate, opt.cnn_optim_alpha, opt.cnn_optim_beta, opt.optim_epsilon, cnn_optim_state)\n",
    "    else\n",
    "      error('bad option for opt.cnn_optim')\n",
    "    end\n",
    "  end\n",
    "\n",
    "  -- print loss and timing/benchmarks\n",
    "  print(string.format('iter %d: %s', iter, utils.build_loss_string(losses)))\n",
    "  if opt.timing then print(utils.build_timing_string(stats.times)) end\n",
    "\n",
    "  if ((opt.eval_first_iteration == 1 or iter > 0) and iter % opt.save_checkpoint_every == 0) or (iter+1 == opt.max_iters) then\n",
    "\n",
    "    -- Set test-time options for the model\n",
    "    model.nets.localization_layer:setTestArgs{\n",
    "      nms_thresh=opt.test_rpn_nms_thresh,\n",
    "      max_proposals=opt.test_num_proposals,\n",
    "    }\n",
    "    model.opt.final_nms_thresh = opt.test_final_nms_thresh\n",
    "\n",
    "    -- Evaluate validation performance\n",
    "    local eval_kwargs = {\n",
    "      model=model,\n",
    "      loader=loader,\n",
    "      split='val',\n",
    "      max_images=opt.val_images_use,\n",
    "      dtype=dtype,\n",
    "\t  gciter=opt.gciter, -- jh\n",
    "    }\n",
    "    local results = eval_utils.eval_split(eval_kwargs)\n",
    "    -- local results = eval_split(1, opt.val_images_use) -- 1 = validation\n",
    "    results_history[iter] = results\n",
    "\n",
    "    -- serialize a json file that has all info except the model\n",
    "    local checkpoint = {}\n",
    "    checkpoint.opt = opt\n",
    "    checkpoint.iter = iter\n",
    "    checkpoint.loss_history = loss_history\n",
    "    checkpoint.results_history = results_history\n",
    "    checkpoint.all_losses = all_losses\n",
    "    checkpoint.best_val_score = best_val_score\n",
    "    checkpoint.iterators = loader.iterators\n",
    "    cjson.encode_number_precision(4) -- number of sig digits to use in encoding\n",
    "    cjson.encode_sparse_array(true, 2, 10)\n",
    "    local text = cjson.encode(checkpoint)\n",
    "    local file = io.open(opt.checkpoint_path .. '.json', 'w')\n",
    "    file:write(text)\n",
    "    file:close()\n",
    "    print('wrote ' .. opt.checkpoint_path .. '.json')\n",
    "\n",
    "    -- Only save t7 checkpoint if there is an improvement in mAP\n",
    "    if best_val_score == nil or results.ap_results.map > best_val_score then\n",
    "      best_val_score = results.ap_results.map\n",
    "      checkpoint.best_val_score = best_val_score\n",
    "      -- save the optim state, for better resuming\n",
    "      checkpoint.optim_state = optim_state\n",
    "      checkpoint.cnn_optim_state = cnn_optim_state\n",
    "      -- save the model\n",
    "      checkpoint.model = model\n",
    "\n",
    "      -- We want all checkpoints to be CPU compatible, so cast to float and\n",
    "      -- get rid of cuDNN convolutions before saving\n",
    "      model:clearState()\n",
    "      model:float()\n",
    "      if cudnn then\n",
    "        cudnn.convert(model.net, nn)\n",
    "        cudnn.convert(model.nets.localization_layer.nets.rpn, nn)\n",
    "      end\n",
    "      torch.save(opt.checkpoint_path, checkpoint)\n",
    "      print('wrote ' .. opt.checkpoint_path)\n",
    "\n",
    "      -- Now go back to CUDA and cuDNN\n",
    "      model:cuda()\n",
    "      if cudnn then\n",
    "        cudnn.convert(model.net, cudnn)\n",
    "        cudnn.convert(model.nets.localization_layer.nets.rpn, cudnn)\n",
    "      end\n",
    "\n",
    "      -- All of that nonsense causes the parameter vectors to be reallocated, so\n",
    "      -- we need to reallocate the params and grad_params vectors.\n",
    "      params, grad_params, cnn_params, cnn_grad_params = model:getParameters()\n",
    "    end\n",
    "  end\n",
    "    \n",
    "  -- stopping criterions\n",
    "  iter = iter + 1\n",
    "  -- Collect garbage every so often\n",
    "  if iter % 33 == 0 then collectgarbage() end\n",
    "  if loss0 == nil then loss0 = losses.total_loss end\n",
    "  if losses.total_loss > loss0 * 100 then\n",
    "    --print('loss seems to be exploding, quitting.')\n",
    "    --break\n",
    "  end\n",
    "  if opt.max_iters > 0 and iter >= opt.max_iters then break end\n",
    "end\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iTorch",
   "language": "lua",
   "name": "itorch"
  },
  "language_info": {
   "name": "lua",
   "version": "5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
